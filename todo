# repo
- armar el pipeline para un modelo base (random forest)
	- Preprocesamiento de los datos: entradas con path, salida lista para extraer features
	- Extraccion de features: entrada datos preprocesados, salida triada (dato raw, embedding de bert, etiquetas) en forma de dataframe
		- instancia raw --> tokens BERT --> token_ids y etiquetas (punt_inicial, punt_final, capitalizacion)
	- Armar la estructura de los configs para controlar:
		- que datos se usan para entrenar y validar
		- que modelo se entrena, y con que hiperparametros
		- que tarea se entrena, y parametros de entrenamiento (epocas, baches, criterios de convergencia, etc)
	- Armar el codigo de validacion, que reciba un modelo entrenado, datos de testeo y configuracion de una tarea. La 
	salida se guarda en resultados con la informacion del modelo usado como de la tarea sobre la que se testea y los datos.
	- Armar codigo para inferencia sobre nuevos datos, pensado en la evaluacion que se indica en el tp
- probar el pipeline para un modelo secuencial (RNN simple)
- sumar notebooks de análisis de datos para spliting y estadísticas de cada problema

# preguntas
- Como afecta el tamaño de la secuencia a la prediccion?
- Como afectan los tipos de datos de train?