# repo
- armar el pipeline para un modelo base (random forest)
	- Preprocesamiento de los datos: entradas con path, salida lista para extraer features
	- Extraccion de features: entrada datos preprocesados, salida triada (dato raw, embedding de bert, etiquetas) en forma de dataframe
		- instancia raw --> tokens BERT --> token_ids y etiquetas (punt_inicial, punt_final, capitalizacion)
	- Armar la estructura de los configs para controlar:
		- que datos se usan para entrenar y validar
		- que modelo se entrena, y con que hiperparametros
		- que tarea se entrena, y parametros de entrenamiento (epocas, baches, criterios de convergencia, etc)
	- Armar el codigo de validacion, que reciba un modelo entrenado, datos de testeo y configuracion de una tarea. La 
	salida se guarda en resultados con la informacion del modelo usado como de la tarea sobre la que se testea y los datos.
	- Armar codigo para inferencia sobre nuevos datos, pensado en la evaluacion que se indica en el tp
- probar el pipeline para un modelo secuencial (RNN simple)
- sumar notebooks de análisis de datos para spliting y estadísticas de cada problema

# preguntas
- Como nos conviene segmentar? Lo que se me ocurre es definir un largo maximo que tenga relacion con el modelo downstream (por ejemplo 64 tokens)
y tomar ventanas de ese largo con un salto (de por ejemplo media ventana). El tema es que habria que limitar las ventanas para que no corten palabras,
es decir, que cada ventana final comience con el comienzo de una palabra, y termine con el final de una palabra. Tiene sentido?
- Como trabajamos las secuencias para los modelos no-secuenciales? Hacemos promedio de embeddings? La idea es probar distintos poolings?
- trabajamos con los embeddings estaticos para empezar. Tiene sentido sacar embeddings contextuales? Como hacemos el finetuneo en ese caso? 