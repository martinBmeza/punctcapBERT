{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b23b8f",
   "metadata": {},
   "source": [
    "# Filtrado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37435095",
   "metadata": {},
   "source": [
    "## Carga de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7636169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              instance_id  \\\n",
      "0  modified_WilliamShakespeareHamlet_p000   \n",
      "1  modified_WilliamShakespeareHamlet_p001   \n",
      "2  modified_WilliamShakespeareHamlet_p002   \n",
      "3  modified_WilliamShakespeareHamlet_p003   \n",
      "4  modified_WilliamShakespeareHamlet_p004   \n",
      "\n",
      "                                            sentence  \n",
      "0  Libro descargado en www.elejandria.com, tu sit...  \n",
      "1                                    dominio público  \n",
      "2                      ¡Esperamos que lo disfrutéis!  \n",
      "3                                             Hamlet  \n",
      "4                                                Por  \n",
      "         instance_id                                           sentence\n",
      "0  la_dama_boba_s000  La dama boba Lope de Vega  Biblioteca Virtual ...\n",
      "1  la_dama_boba_s001  Accesible desde http://cervantesvirtual.com Añ...\n",
      "2  la_dama_boba_s002                              PERSONAJES caballero.\n",
      "3  la_dama_boba_s003                                            lacayo.\n",
      "4  la_dama_boba_s004                                         caballero.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Emilianohack6950/Wikipedia-es\")\n",
    "\n",
    "df_hamlet = pd.read_csv('../data/raw/hamlet_sentences.csv')\n",
    "print(df_hamlet.head())\n",
    "\n",
    "df_dama_boba = pd.read_csv('../data/raw/la_dama_boba_sentences.csv')\n",
    "print(df_dama_boba.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc6fed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el split 'train' del dataset ds a un DataFrame\n",
    "df_wikipedia_train = ds['train'].to_pandas()\n",
    "\n",
    "# Convertir el split 'test' del dataset ds a un DataFrame\n",
    "df_wikipedia_test = ds['test'].to_pandas()\n",
    "\n",
    "# Unir ambos DataFrames\n",
    "df_wikipedia_combined = pd.concat([df_wikipedia_train, df_wikipedia_test], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd8d85",
   "metadata": {},
   "source": [
    "## Filtro de oraciones\n",
    "Se filtran las oraciones que no tienen signos de puntuación además del punto final, ni mayúsculas además de la primera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508faa58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27530440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/santiago/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Usamos nltk para que nos divida en oraciones de una forma simple sin tener que escribir el código a mano\n",
    "nltk.download('punkt', force=True)\n",
    "# Some NLTK versions store a different Spanish sentence tokenizer under 'punkt_tab'\n",
    "# Download it proactively to avoid the \"Resource punkt_tab not found\" error\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "\n",
    "def sentences_from_dataframe(df: pd.DataFrame, column: str, language: str = 'spanish') -> list:\n",
    "    \"\"\"Une la columna y segmenta en oraciones con nltk (si ya tenés oraciones, no hace falta usarla).\"\"\"\n",
    "    text = df[column].astype(str).str.cat(sep=' ')\n",
    "    return nltk.tokenize.sent_tokenize(text, language=language)\n",
    "\n",
    "\n",
    "def is_interesting_sentence(sent: str) -> bool:\n",
    "    \"\"\"Return True if the sentence contains punctuation besides a final-only '.'\n",
    "    or contains uppercase characters beyond the first character.\n",
    "\n",
    "    The filtering rule implemented matches your description:\n",
    "    - Keep sentences that contain any punctuation other than a final '.' (e.g. commas, ?, ¡, ¿, ;, etc.)\n",
    "    - OR keep sentences that have uppercase letters after the first character\n",
    "\n",
    "    Sentences that have neither are filtered out.\n",
    "    \"\"\"\n",
    "    s = sent.strip()\n",
    "    if len(s) == 0:\n",
    "        return False\n",
    "\n",
    "    # Check uppercase beyond the first character\n",
    "    if any(ch.isupper() for ch in s[1:]):\n",
    "        return True\n",
    "\n",
    "    # Check punctuation other than a final-only '.'\n",
    "    # punctuation chars to consider (excluding the dot)\n",
    "    punct_re = re.compile(r'¿,?')\n",
    "    if punct_re.search(s):\n",
    "        return True\n",
    "\n",
    "    # If there is a dot anywhere except the final position, consider interesting\n",
    "    if '.' in s[:-1]:\n",
    "        return True\n",
    "\n",
    "    # If none of the above, sentence is uninteresting (will be filtered out)\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_sentences(sentences: list) -> list:\n",
    "    \"\"\"Return a list of sentences that are \"interesting\" per is_interesting_sentence().\"\"\"\n",
    "    return [s for s in sentences if is_interesting_sentence(s)]\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, column: str, language: str = 'spanish', max_sentences: int = None, apply_filter: bool = False) -> list:\n",
    "    \"\"\"Conveniencia: extrae oraciones de un DataFrame y las procesa (usa sentences_from_dataframe).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame that contains the text column.\n",
    "        column: name of the column with textual content.\n",
    "        language: language for tokenization (default 'spanish').\n",
    "        max_sentences: optional limit for number of sentences to return.\n",
    "        apply_filter: if True, keeps only sentences considered \"interesting\".\n",
    "\n",
    "    Returns:\n",
    "        list of sentences (strings).\n",
    "    \"\"\"\n",
    "    sents = sentences_from_dataframe(df, column, language=language)\n",
    "\n",
    "    if apply_filter:\n",
    "        sents = filter_sentences(sents)\n",
    "\n",
    "    if max_sentences is not None:\n",
    "        sents = sents[:max_sentences]\n",
    "\n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b8e9428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11774, 14817)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_wiki = process_dataframe(df_wikipedia_combined, \"contenido\")\n",
    "len(processed_wiki), len(sentences_from_dataframe(df_wikipedia_combined, \"contenido\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25264884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1421, 3650)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_hamlet = process_dataframe(df_hamlet, \"sentence\")\n",
    "len(processed_hamlet), df_hamlet.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "788ecb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(953, 1725)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dama = process_dataframe(df_dama_boba, \"sentence\")\n",
    "len(processed_dama), df_dama_boba.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
